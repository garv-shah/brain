<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="This section of the Algorithmics SAT focuses on a time complexity analysis of the solution in order to establish the efficiency of the algorithm and feasibility in the real world."><meta property="og:title" content="Algorithmics SAT - Friendship Network Part 2"><meta property="og:description" content="This section of the Algorithmics SAT focuses on a time complexity analysis of the solution in order to establish the efficiency of the algorithm and feasibility in the real world."><meta property="og:type" content="website"><meta property="og:image" content="https://quartz.jzhao.xyz/icon.png"><meta property="og:url" content="https://quartz.jzhao.xyz/notes/School-Subjects/Algorithmics/SAT/Part-2/Algorithmics-SAT-Part-2/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Algorithmics SAT - Friendship Network Part 2"><meta name=twitter:description content="This section of the Algorithmics SAT focuses on a time complexity analysis of the solution in order to establish the efficiency of the algorithm and feasibility in the real world."><meta name=twitter:image content="https://quartz.jzhao.xyz/icon.png"><title>Algorithmics SAT - Friendship Network Part 2</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://quartz.jzhao.xyz//icon.png><link href=https://quartz.jzhao.xyz/styles.f8783aeb83f6dafe71465d8b7c8025c9.min.css rel=stylesheet><link href=https://quartz.jzhao.xyz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://quartz.jzhao.xyz/js/darkmode.63d6a3e095d0bd2b935b62adec9dc11b.min.js></script>
<script src=https://quartz.jzhao.xyz/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://quartz.jzhao.xyz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://quartz.jzhao.xyz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://quartz.jzhao.xyz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://quartz.jzhao.xyz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://quartz.jzhao.xyz/",fetchData=Promise.all([fetch("https://quartz.jzhao.xyz/indices/linkIndex.ea8c34a0dacf5f7a36972f15d71e6b74.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://quartz.jzhao.xyz/indices/contentIndex.ce6bd0f66ff2b18b8917bcdb942082c3.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://quartz.jzhao.xyz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://quartz.jzhao.xyz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/quartz.jzhao.xyz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=quartz.jzhao.xyz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://quartz.jzhao.xyz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://quartz.jzhao.xyz/>ðŸ‘‹ Garv's Notes</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Algorithmics SAT - Friendship Network Part 2</h1><p class=meta>Last updated
Jul 28, 2023
<a href=https://github.com/garv-shah/notes/tree/hugo/content/notes/School%20Subjects/Algorithmics/SAT/Part%202/Algorithmics%20SAT%20Part%202.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#expected-time-complexity>Expected Time Complexity</a></li><li><a href=#call-tree>Call Tree</a></li><li><a href=#held-karp-time-complexity>Held-Karp Time Complexity</a></li><li><a href=#dijkstras-time-complexity>Dijkstra&rsquo;s Time Complexity</a></li><li><a href=#modified-held-karp-time-complexity>Modified Held-Karp Time Complexity</a><ol><li><a href=#recurrence-relation>Recurrence Relation</a></li><li><a href=#attempting-to-find-an-explicit-formula>Attempting to Find an Explicit Formula</a></li><li><a href=#time-complexity>Time Complexity</a></li></ol></li><li><a href=#optimised-modified-held-karp-time-complexity>Optimised Modified Held-Karp Time Complexity</a></li></ol><ol><li><a href=#revisiting-problem-requirements>Revisiting Problem Requirements</a></li></ol><ol><li><a href=#possible-optimisations>Possible Optimisations</a></li><li><a href=#algorithm-pseudocode>Algorithm Pseudocode</a><ol><li><a href=#main-function>Main Function</a></li><li><a href=#calculate-nodes>Calculate Nodes</a></li><li><a href=#held-karp>Held-Karp</a></li><li><a href=#dijkstras>Dijkstra&rsquo;s</a></li><li><a href=#distance-function>Distance Function</a></li></ol></li></ol></nav></details></aside><p>This section of the Algorithmics SAT focuses on a time complexity analysis of the solution in order to establish the efficiency of the algorithm and feasibility in the real world.</p><p>Throughout the analysis, note the following variables are used as shorthand:</p><p>Let $F =$ number of friends</p><p>Let $L =$ number of landmarks</p><p>Let $R =$ number of routes</p><a href=#time-complexity-analysis><h1 id=time-complexity-analysis><span class=hanchor arialabel=Anchor># </span>Time Complexity Analysis</h1></a><a href=#expected-time-complexity><h2 id=expected-time-complexity><span class=hanchor arialabel=Anchor># </span>Expected Time Complexity</h2></a><p>As explained in Part 1 of the SAT, the algorithm in essence boils down to an applied version of the Heldâ€“Karp algorithm, which has an optimal worst case time complexity of $O(n^{2}2^{n})$. Hence, it would make sense for our combination of Held-Karp and Dijkstra&rsquo;s to result in a time complexity slightly larger.</p><a href=#call-tree><h2 id=call-tree><span class=hanchor arialabel=Anchor># </span>Call Tree</h2></a><p><img src=https://quartz.jzhao.xyz//call_tree.svg width=auto alt="Call Tree" title="Call Tree"></p><p>As we can see, the
<a rel=noopener class="internal-link broken" data-src=#main-function>main function</a> calls a few distinct processes <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:</p><ol><li><p>First it creates the edge lookup matrix, which is abstracted in the pseudocode. This Big O time is derived from the Pythonic implementation of the lookup matrix as follows <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>edge_lookup_matrix</span> <span class=o>=</span> <span class=p>{</span><span class=nb>frozenset</span><span class=p>({</span><span class=n>edge</span><span class=p>[</span><span class=s1>&#39;from&#39;</span><span class=p>],</span> <span class=n>edge</span><span class=p>[</span><span class=s1>&#39;to&#39;</span><span class=p>]}):</span> <span class=p>[]</span> <span class=k>for</span> <span class=n>edge</span> <span class=ow>in</span> <span class=n>edges</span><span class=p>}</span>  
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>edge</span> <span class=ow>in</span> <span class=n>edges</span><span class=p>:</span>  
</span></span><span class=line><span class=cl>	<span class=n>edge_lookup_matrix</span><span class=p>[</span><span class=nb>frozenset</span><span class=p>({</span><span class=n>edge</span><span class=p>[</span><span class=s1>&#39;from&#39;</span><span class=p>],</span> <span class=n>edge</span><span class=p>[</span><span class=s1>&#39;to&#39;</span><span class=p>]})]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>edge</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>``</p><p>Evidently, this loops over each edge in <code>edges</code> twice, resulting in a linear time complexity of $O(2R)$</p></li><li><p>It then calls
<a rel=noopener class="internal-link broken" data-src=#calculate-nodes><code>calculate_nodes</code></a> with an input of both <code>friends</code> and <code>landmarks</code>, the output of which is used to create our <code>visit_set</code>. This Big O time is derived from the fact that
<a rel=noopener class="internal-link broken" data-src=#calculate-nodes><code>calculate_nodes</code></a> is simply a nested for-loop, iterating over each friend and every landmark, resulting in a worst case time complexity of $O(F\times L)$.</p></li><li><p>It now uses the output of
<a rel=noopener class="internal-link broken" data-src=#calculate-nodes><code>calculate_nodes</code></a> (stored as <code>friend_distances</code>) to create a set of nodes we need to visit, which is abstracted in the pseudocode. This Big O time is derived from the Pythonic implementation of the set as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>visit_set</span> <span class=o>=</span> <span class=nb>set</span><span class=p>(</span><span class=n>val</span><span class=p>[</span><span class=s1>&#39;closest_node&#39;</span><span class=p>]</span> <span class=k>for</span> <span class=n>key</span><span class=p>,</span> <span class=n>val</span> <span class=ow>in</span> <span class=n>friend_distances</span><span class=o>.</span><span class=n>items</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><p>``</p><p>Evidently, this loops over each friend once, resulting in a linear time complexity of $O(F)$</p></li><li><p>Similar to the above implementation, the
<a rel=noopener class="internal-link broken" data-src=#main-function><code>main</code></a> function now creates <code>people_at_nodes</code> to create a dictionary of nodes and which people are closest to that node, with a similar $O(F)$ as above.</p></li><li><p>Various other print statements are called, all with $O(F)$ time to display information about each friend.</p></li><li><p>Finally, after all this prep is done,
<a rel=noopener class="internal-link broken" data-src=#held-karp><code>held_karp</code></a> is called to find the shortest hamiltonian path of the graph.</p></li></ol><p>As we can see from this process and the call tree above, there are 3 main elements that contribute to the time complexity of our algorithm besides
<a rel=noopener class="internal-link broken" data-src=#held-karp><code>held_karp</code></a>:</p><ol><li><p><a rel=noopener class="internal-link broken" data-src=#calculate-nodes><code>calculate_nodes</code></a> which contributes $F\times L$ to our time.</p></li><li><p>Calculating the <code>edge_lookup_matrix</code>, which contributes $2R$ to our time complexity but simply turns into $R$ when considering the asymptotic complexity.</p></li><li><p>Calculating the <code>visit_set</code>, <code>people_at_nodes</code> and two other print calls. This contributes $4F$ where 4 accounts for these 4 processes but could be any other arbitrary constant, as this simply turns into $F$ when considering the asymptotic time complexity.</p></li></ol><p>If we let the time complexity of
<a rel=noopener class="internal-link broken" data-src=#held-karp><code>held_karp</code></a> be represented by $HK(n)$ where $n$ denotes the calculated size of the <code>visit_set</code>, our current time complexity of the
<a rel=noopener class="internal-link broken" data-src=#main-function><code>main</code></a> function can be represented as $O(HK(n)+FL+R+F)$.</p><a href=#held-karp-time-complexity><h2 id=held-karp-time-complexity><span class=hanchor arialabel=Anchor># </span>Held-Karp Time Complexity</h2></a><p>Figuring out the time complexity of the other processes in our algorithm was relatively easy; we can simply look at their
<a rel=noopener class="internal-link broken" data-src=#algorithm-pseudocode>pseudocode implementation</a> (or what they would be if they are abstracted) and look at the general number of operations. Held-Karp on the other hand is a bit harder as it is a recursive algorithm, making direct analysis a bit more troublesome. To begin, we can try to represent the
<a rel=noopener class="internal-link broken" data-src=#held-karp>modified Held-Karp algorithm</a> as a recurrence relation to aid in mathematical analysis.</p><p>To recap, Held-Karp<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> works by utilising the fact that every subpath of a path of minimum distance is itself of minimum distance. This means that we can reduce the length of $S$ by one each time by finding the minimum distance/path between $C$ and $B$ while running Held-Karp again on the set $S$ without $C$, but as $C$ as the new value for $B$.</p><p>As stated in part 1, this logic can be represented recursively as the following:</p><p>$\textrm{Let Cost}_{A \rightarrow B, \space S}=\textrm{The minimum cost of a cycle free path from A to B that visits all the vertices of S.}$</p><p>$\textrm{Let } d_{A,B} = \textrm{The minimum cost of travelling from A to B, as outputted by Dijkstra&rsquo;s.}$</p><p>$\therefore \textrm{Cost}<em>{A \rightarrow B, \space S}= \textrm{min}(\textrm{Cost}</em>{A \rightarrow C, \space S - {B}} + d_{CB})$</p><p>We can then turn this into a recurrence relation for Big O, where $n$ is the size of the set $S$ and $d(n)$ is the cost function, which in our case is Dijkstra&rsquo;s:</p><p>$$
T_{n} = \left{
\begin{array}{ll}
n(T_{n-1}+d(n)) & \quad n > 0 \\ d(n) & \quad n=0
\end{array}
\right.
$$</p><p>Now that we have a recurrence relation for Held-Karp in terms of the cost of running Dijkstra&rsquo;s, the next logical step is to find the number of operations required to run Dijkstra&rsquo;s every time (which would be in the worst case scenario where none of our previous calculations are reused).</p><a href=#dijkstras-time-complexity><h2 id=dijkstras-time-complexity><span class=hanchor arialabel=Anchor># </span>Dijkstra&rsquo;s Time Complexity</h2></a><p>We can analyse Dijkstra&rsquo;s step by step by viewing all the elements of the
<a rel=noopener class="internal-link broken" data-src=#dijkstras>pseudocode</a> and evaluating them separately and then add them up together at the end:</p><ol><li><p>We can see that initial loop runs for every node, or $L$ times, as each node represents a landmark.</p></li><li><p>In the main while loop, we iterate over every node in the graph, making the while loop run $L$ times as well.</p></li><li><p>To find the <code>min_node</code>, the pseudocode iterates over every single node in the <code>unexplored_list</code>. As this list decreases by one each time, the total cost of finding the <code>min_node</code> can be represented as $L+(L-1)+(L-2)+\cdots+1+0$. This resembles the triangular numbers, and hence we can also represent the total <code>min_node</code> cost as $\frac{L(L+1)}{2}$.</p></li><li><p>The nested for loop inside the while loop is a bit trickier as it covers all neighbours of the current <code>min_node</code>. As we have established that every single node in the graph will be the <code>min_node</code> at some point, we can use the graph below as an example for how many times this loop would occur.
Over here, we can see that $A$ has 2 neighbours, $B$ has 2 neighbours, $C$ has 1 neighbour and $D$ has 1 neighbour. This makes it evident that the amount of times this inner for loop will run is actually just the sum of the degrees of the graph, and by the handshaking lemma, this is simply equal to twice the number of edges in the graph. Hence, the total amount of times this loop will run is $2R$.</p><p><img src=https://quartz.jzhao.xyz//sample_graph.svg width=auto alt="Sample Graph" title="Sample Graph"></p></li><li><p>Finally, inside this for loop, we call the <code>dist</code> function. As is evident from the pseudocode, this function uses the <code>edge_lookup_matrix</code> and goes over the edges between two nodes. In most practical cases, this will simply be one or two edges if multiple bus or train lines go across the same nodes. The <code>soonest_time_at_node</code> function is also an abstraction the next available bus/train time given any time at a particular node, which can possibly be implemented into a dictionary to be done in constant time. Due to these two factors, when looking at the asymptotic behaviour, this can be simplified to $O(1)$.</p></li></ol><p>Now that we have considered all parts of our implementation of Dijkstra&rsquo;s, we can combine it to get a single cost function: $d(n) = L + L\left(\frac{L(L+1)}{2}+2R\right)= 2LR+\frac{1}{2}L^{3}+\frac{1}{2}L^{2}+L$. Considering the behaviour of this function asymptotically, we can see that it would have a time complexity of $O(2LR + L^{3})$, which is far from ideal and can be improved significantly (Dijkstra&rsquo;s can supposedly be done in $O(L+R\log{L})$ with a min-priority queue).</p><a href=#modified-held-karp-time-complexity><h2 id=modified-held-karp-time-complexity><span class=hanchor arialabel=Anchor># </span>Modified Held-Karp Time Complexity</h2></a><p>Now that we have an established cost function, we can attempt to evaluate $T_{n}$ in terms of $d(n)$. To reiterate:</p><p>$$
T_{n} = \left{
\begin{array}{ll}
n(T_{n-1}+d(n)) & \quad n > 0 \\ d(n) & \quad n=0
\end{array}
\right.
$$
$$
d(n)=2LR+\frac{1}{2}L^{3}+\frac{1}{2}L^{2}+L
$$</p><p>Keeping this in terms of $d(n)$, we can create a table to see how this recurrence relation gets bigger as $n$ increases.</p><table><thead><tr><th>$n$</th><th>$T_{n}$</th></tr></thead><tbody><tr><td>0</td><td>$d(n)$</td></tr><tr><td>1</td><td>$2d(n)$</td></tr><tr><td>2</td><td>$6d(n)$</td></tr><tr><td>3</td><td>$21d(n)$</td></tr><tr><td>4</td><td>$88d(n)$</td></tr><tr><td>5</td><td>$445d(n)$</td></tr></tbody></table><p>The working for this table is shown below, but you can easily keep going to follow the pattern for higher values of $n$:</p><p>$n = 0$: $T_{n}=d(n)$</p><p>$n = 1$: $T_{n}=1(T_{0}+d(n))=2d(n)$</p><p>$n = 2$: $T_{n}=2(T_{1}+d(n))=6d(n)$</p><p>$n = 3$: $T_{n}=3(T_{2}+d(n))=21d(n)$</p><p>$n = 4$: $T_{n}=4(T_{3}+d(n))=88d(n)$</p><p>$n = 5$: $T_{n}=5(T_{4}+d(n))=445d(n)$</p><a href=#recurrence-relation><h3 id=recurrence-relation><span class=hanchor arialabel=Anchor># </span>Recurrence Relation</h3></a><p>Just looking at the coefficients for a second, we have the following recurrence relation:</p><p>$$
T_{n}=n(T_{n-1}+1), \space T_{0}=1
$$</p><p>It is easy to see that this recurrence relation implies that the running time for the algorithm is factorial. After all, the recurrence relation for $n!$ is $T_{n}=n(T_{n-1}), \space T_{0}=1$.</p><a href=#attempting-to-find-an-explicit-formula><h3 id=attempting-to-find-an-explicit-formula><span class=hanchor arialabel=Anchor># </span>Attempting to Find an Explicit Formula</h3></a><p>Now clearly it is of interest to solve this
<a rel=noopener class="internal-link broken" data-src=#recurrence-relation>recurrence relation</a> and find a non-recursive formula, and here we run into a bit of an issue. If the relation were a linear recurrence with constant coefficients or a typical divide-and conquer recurrence, it would likely be solvable by well-known methods such as telescoping or the Master Theorem, but this is not the case.</p><a href=#theorem-1><h4 id=theorem-1><span class=hanchor arialabel=Anchor># </span>Theorem 1</h4></a><p>While trying to find a way to solve this
<a rel=noopener class="internal-link broken" data-src=#recurrence-relation>recurrence relation</a>, I arrived at the conjecture that $T_{n}=n!+\sum_{i=0}^{n-1} \frac{n!}{i!}$, so let us try to prove it.</p><blockquote><p>For $n \in \mathbb{N}$, the number of operations used to solve an n-sized visit set TSP by the above algorithm (ignoring the cost function) satisfied the formula: $T_{n}=n!+\sum_{i=0}^{n-1} \frac{n!}{i!}$.</p></blockquote><p>First let us work with the RHS to rearrange it a bit into a more convenient form:
$RHS$</p><p>$= n!+\sum_{i=0}^{n-1} \frac{n!}{i!}$</p><p>$= n!+\frac{n!}{0!}+\frac{n!}{1!}+\frac{n!}{2!}+\cdots+\frac{n!}{(n-2)!}+\frac{n!}{(n-1)!}$</p><p>$= n! \times (1 + \frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\cdots+\frac{1}{(n-2)!}+\frac{1}{(n-1)!})$</p><a href=#base-case><h5 id=base-case><span class=hanchor arialabel=Anchor># </span>Base Case</h5></a><p>When $n=0$, the base case of the
<a rel=noopener class="internal-link broken" data-src=#recurrence-relation>recurrence relation</a> says that $T_{0}=1$. The above formula matches that with $T_{0}=0!\times(1+0)=1$, $\therefore$ base case is true.</p><a href=#induction-step><h5 id=induction-step><span class=hanchor arialabel=Anchor># </span>Induction Step</h5></a><p>Pick an arbitrary $k \in \mathbb{N}$. Assume that the theorem holds for any TSP with a visit set of size $k$. Thus, it is assumed that $T_{k}= k! \times (1 + \frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\cdots+\frac{1}{(k-2)!}+\frac{1}{(k-1)!})$.</p><p>Proof by induction requires showing the following:</p><p>$T_{k+1}= (k+1)! \times (1 + \frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\cdots+\frac{1}{(k-1)!}+\frac{1}{k!})$.</p><p>Next, we can combine the recurrence above with the induction hypothesis as follows:</p><p>$LHS$</p><p>$=T_{k+1}$</p><p>$=T_{k}(k+1)+(k+1)$ (from
<a rel=noopener class="internal-link broken" data-src=#recurrence-relation>recurrence relation</a>)</p><p>$=
<a rel=noopener class="internal-link broken" data-src=k+1>k! \times (1 + \frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\cdots+\frac{1}{(k-2)!}+\frac{1}{(k-1)!})</a>+(k+1)$</p><p>$=(k+1)! \times (1 + \frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\cdots+\frac{1}{(k-2)!}+\frac{1}{(k-1)!})+(k+1)$</p><p>$=(k+1)! \times (1 + \frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\cdots+\frac{1}{(k-2)!}+\frac{1}{(k-1)!})+(k+1)\times\frac{(k+1)!}{(k+1)!}$</p><p>$=(k+1)! \times \left(1 + \frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\cdots+\frac{1}{\left(k-2\right)!}+\frac{1}{(k-1)!}+\frac{k+1}{(k+1)!}\right)$</p><p>$=(k+1)! \times \left(1 + \frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\cdots+\frac{1}{\left(k-2\right)!}+\frac{1}{(k-1)!}+\frac{1}{k!}\right)$</p><p>$=RHS$</p><p>Thus $T_{n}=n!+\sum_{i=0}^{n-1} \frac{n!}{i!}$ by the principle of mathematical induction.</p><a href=#theorem-2><h4 id=theorem-2><span class=hanchor arialabel=Anchor># </span>Theorem 2</h4></a><p>Looking all over the web for this, the only place I could find any reference to this sequence is
<a href=https://oeis.org/A033540 rel=noopener>here</a>, which provides us with the relation $T_{n}=n! + \lfloor e\times n!\rfloor - 1$ for the coefficients. This can be rearranged to $T_{n}=\lfloor n!(e+1)-1 \rfloor$, but just to be sure that this works for every case, we should probably prove it too.</p><blockquote><p>For $n \in \mathbb{Z}^{+}$, the number of operations used to solve an n-sized visit set TSP by the above algorithm (ignoring the cost function) satisfied the formula: $T_{n}=\lfloor n!(e+1)-1 \rfloor$.</p></blockquote><a href=#case-1><h5 id=case-1><span class=hanchor arialabel=Anchor># </span>Case 1</h5></a><p>This is the case where $n=1$. As seen above, $T_{1}=2$ and the proposed formula predicts that $T_{1}= \lfloor 1!(e+1)-1 \rfloor = \lfloor e+1-1 \rfloor = \lfloor e \rfloor = 2$. Thus, the base case holds.</p><a href=#case-2><h5 id=case-2><span class=hanchor arialabel=Anchor># </span>Case 2</h5></a><p>This is the case where $n>1$. Because of the floor function, if it can be shown that the following difference is small enough, it will probably be possible to prove that this case works as well.
$$
\textrm{Let } \space r_{n}=n!(e+1)-1-T_{n}
$$</p><a href=#lemma-1><h5 id=lemma-1><span class=hanchor arialabel=Anchor># </span>Lemma 1</h5></a><blockquote><p>When $n>1$, the following must be true: $r_{n}=\frac{1}{n+1}+\frac{1}{(n+1)(n+2)}+\frac{1}{(n+1)(n+2)(n+3)}+\cdots$</p></blockquote><p>This sum looks like it might be related to the power series for $e^{x}$ at $x=1$. We already know the power series for $e^{x}$, a proof for which can be found
<a href=https://proofwiki.org/wiki/Power_Series_Expansion_for_Exponential_Function rel=noopener>here</a>:</p><p>$$
e^{x}=\frac{1}{0!}+\frac{x}{1!}+\frac{x^{2}}{2!}+\frac{x^{3}}{3!}+\cdots
$$</p><p>It therefore follows that:</p><p>$$
e=e^{1}=\frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\frac{1}{3!}+\cdots
$$
Since we know that $T_{n} = n! \times (1 + \frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\cdots+\frac{1}{(n-2)!}+\frac{1}{(n-1)!})$ from the
<a rel=noopener class="internal-link broken" data-src=#theorem-1>first theorem</a>, we can sub both the power series for $e$ and this fact into our definition of $r_{n}$:</p><p>$r_{n}$</p><p>$= n!(e+1)-1-T_{n}$ (by definition)</p><p>$= n!(1+\frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\cdots)-1-T_{n}$ (power series for $e$)</p><p>$= n!(1+\frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\cdots)-1-n! \times (1 + \frac{1}{0!}+\frac{1}{1!}+\cdots+\frac{1}{(n-2)!}+\frac{1}{(n-1)!})$</p><p>$=n!\times(\frac{1}{n!}+\frac{1}{(n+1)!}+\frac{1}{(n+2)!}+\cdots)-1$</p><p>$=(1+\frac{1}{n+1}+\frac{1}{(n+1)(n+2)}+\cdots)-1$</p><p>$=\frac{1}{n+1}+\frac{1}{(n+1)(n+2)}+\frac{1}{(n+1)(n+2)(n+3)}+\cdots$</p><p>$\therefore$ The lemma is true.</p><a href=#lemma-2><h5 id=lemma-2><span class=hanchor arialabel=Anchor># </span>Lemma 2</h5></a><blockquote><p>When $n>1$, it is true that $r_{n}&lt;\frac{1}{n+1}+\frac{1}{(n+1)^{2}}+\frac{1}{(n+1)^{3}}+\cdots=\frac{1}{n}$</p></blockquote><p>This is easily proven using the
<a rel=noopener class="internal-link broken" data-src=#lemma-1>first lemma</a>:</p><p>$r_{n}$</p><p>$=\frac{1}{n+1}+\frac{1}{(n+1)(n+2)}+\frac{1}{(n+1)(n+2)(n+3)}+\cdots$ (Lemma 1)</p><p>$&lt; \frac{1}{n+1}+\frac{1}{(n+1)(n+1)}+\frac{1}{(n+1)(n+1)(n+1)}+\cdots$</p><p>$= \frac{1}{n+1}+\frac{1}{(n+1)^{2}}+\frac{1}{(n+1)^{3}}+\cdots$</p><p>This upper bound above is in the form of an infinite geometric series with ratio $\frac{1}{n+1}$, so the usual formula of $S_{\infty}=\frac{a}{1-r}$ can be used: $r_{n} &lt;\frac{\frac{1}{n+1}}{1-\frac{1}{n+1}} =\frac{1}{n}$.</p><p>$\therefore$ The lemma is true.</p><a href=#lemma-3><h5 id=lemma-3><span class=hanchor arialabel=Anchor># </span>Lemma 3</h5></a><blockquote><p>If $n > 1$, $0&lt;r_{n}&lt;1$ must hold true.</p></blockquote><p>From
<a rel=noopener class="internal-link broken" data-src=#lemma-1>Lemma 1</a>, it is clear that $r_{n}$ is positive $\therefore 0&lt; r_{n}$.</p><p>Then, by
<a rel=noopener class="internal-link broken" data-src=#lemma-2>Lemma 2</a>, the following must hold: $r_{n}&lt;\frac{1}{n}\le\frac{1}{2}&lt;1$. $\therefore r_{n}&lt;1$.</p><p>$\therefore$ The lemma is true.</p><a href=#conclusion><h5 id=conclusion><span class=hanchor arialabel=Anchor># </span>Conclusion</h5></a><p>Thus, the proof for this theorem is complete for the case $n>1$:</p><p>By the
<a rel=noopener class="internal-link broken" data-src=#case-2>definition</a> of $r_{n}$, it must be true that $T_{n}+r_{n}=n!(e+1)-1$. Since the
<a rel=noopener class="internal-link broken" data-src=#recurrence-relation>recurrence relation</a> set up $T_{n}$ as integer and $0&lt;r_{n}&lt;1$ by
<a rel=noopener class="internal-link broken" data-src=#lemma-3>Lemma 3</a>, it must hold that $\lfloor n!(e+1)-1 \rfloor = \lfloor T_{n}+r_{n} \rfloor=T_{n}$.</p><a href=#time-complexity><h3 id=time-complexity><span class=hanchor arialabel=Anchor># </span>Time Complexity</h3></a><p>Now that we have proved this works for the coefficients of the cost function, we have the formula of $T(n)=d(n) \lfloor n!(e+1)-1 \rfloor$.The floor function here is just to deal with the difference of $r_{n}$ so that we can get an integer output. Subbing in our known time complexity of $d(n)$, we get a final Big O of $O(\lfloor n!(e+1) \rfloor(2LR+L^{3}))$ for the original implementation of our modified Held-Karp with no caching of its own Dijkstra&rsquo;s outputs. Note that it should already have been obvious that the running time for this algorithm would be in factorial time from the recurrence relation itself, even before finding an explicit formula.</p><p>We have already verified that this is correct given that the recurrence relation is correct, but we can also do so by general intuition . If we look back at Part 1, we can get the time taken to run the unoptimised modified Held-Karp on our data with different $n$ values. $(2LR+L^{3})$ should be a constant for any particular predefined graph, meaning that if our Big O time complexity is correct then $\textrm{execution time} \propto \lfloor n!(e+1) \rfloor$<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><table><thead><tr><th>n</th><th>$\frac{\textrm{execution time}}{\lfloor n!(e+1) \rfloor}$</th></tr></thead><tbody><tr><td>5</td><td>$3\times10^{-5}$</td></tr><tr><td>6</td><td>$4\times10^{-5}$</td></tr><tr><td>7</td><td>$3\times10^{-5}$</td></tr><tr><td>8</td><td>$3\times10^{-5}$</td></tr><tr><td>9</td><td>$3\times10^{-5}$</td></tr></tbody></table><p>As we can see, this proportionality is fairly constant, so it would probably be safe to assume that the worst-case time complexity for the unoptimised modified Held-Karp algorithm would be $O(\lfloor n!(e+1) \rfloor(2LR+L^{3}))$, or at least something pretty close to it.</p><a href=#optimised-modified-held-karp-time-complexity><h2 id=optimised-modified-held-karp-time-complexity><span class=hanchor arialabel=Anchor># </span>Optimised Modified Held-Karp Time Complexity</h2></a><p>As was established in part 1, this factorial time complexity is not nearly sufficient enough for real world applications. Not only is it simply worse than brute forcing it, it makes it so calculating the Hamiltonian path with just my own friend group takes a ludicrous amount of time.</p><p>One optimisation that was made in Part 1 was the caching of Dijkstra&rsquo;s outputs, so that once Dijkstra&rsquo;s is called from one starting node, all subsequent calls to Dijkstra&rsquo;s will be done in $O(1)$ time. This means that the full Dijkstra&rsquo;s algorithm will only be called a maximum of once for every node in the graph, and then all subsequent calls will just use the cache. Since the time complexity for our Dijkstra&rsquo;s implementation is currently $O(2LR + L^{3})$, we can simply multiply this by the amount of nodes ($L$) to get the worst case scenario for how long Dijkstra&rsquo;s takes.</p><p>This transforms our time complexity of $O(\lfloor n!(e+1) \rfloor(2LR+L^{3}))$ into $O(\lfloor n!(e+1) \rfloor + L(2LR+L^{3}))$, which doesn&rsquo;t <em>look</em> like that much of a difference, but it means that when looking at the asymptotic time as $n \to \infty$, we can remove the whole second term as it becomes a constant if we are not considering increasing the amount of landmarks and routes, which is much better than multiplying by this value instead.</p><p>As $n \to \infty$, not only does the 2nd term become negligible as explained above, but the floor function also doesn&rsquo;t make a difference because it is simply for making the output an integer number of operations. As such, it is safe to conclude that the implemented algorithm runs in factorial time for an increasing size of the <code>visit_set</code>.</p><p>In conclusion, the final algorithm from part one has a time complexity of $O(\lfloor n!(e+1) \rfloor + L(2LR+L^{3}))$, which means that the algorithm runs in factorial time.</p><a href=#consequences-of-time-complexity><h1 id=consequences-of-time-complexity><span class=hanchor arialabel=Anchor># </span>Consequences of Time Complexity</h1></a><p>As detailed in the previous section, the final time complexity of the algorithm so far is $O(\lfloor n!(e+1) \rfloor + L(2LR+L^{3}))$. This isn&rsquo;t very ideal, because simply brute forcing it would likely lead to a better worst case time complexity than the current algorithm.</p><p>Let&rsquo;s quickly take the example of the time complexities of our two algorithms, the one with cached Dijkstra&rsquo;s values and the one without. The graph/input data detailed in Part 1 has 15 landmarks, 26 routes and a <code>visit_set</code> of size 7. For these values, the unoptimised algorithm would take 77,864,700 time units and the algorithm with Dijkstra&rsquo;s caching would take 81,065 time units. This is over 960 times faster in the worst case scenario, but as shown in part 1, about 31 times faster in the average case. Below is a discussion on the real world consequences of this time performance difference, as well as how practical this algorithm is for real world use cases.</p><a href=#revisiting-problem-requirements><h2 id=revisiting-problem-requirements><span class=hanchor arialabel=Anchor># </span>Revisiting Problem Requirements</h2></a><p>This algorithm was made to solve the general problem of planning trips with friends, but more specifically the scenario where my friends decided that we want to travel in one big travel party and I am to start and end my day at my house, picking up all my friends along the way. In other words, this algorithm is designed for the real world use case of finding the shortest circuit that picks up all my friends as we travel.</p><p>Let us consider some requirements for this real world use case. By my own general estimates, most people would only have about 5 to 10 close friends that they would travel like this with. Similarly, most people live relatively close to their friends, so the case of 15 landmarks (or train stations/buses) and 26 routes (or train/bus lines) is realistic. As shown in Part 1, below is the real world performance as $n \in [0,12]$ and $L=15,\space R=26$.</p><table><thead><tr><th>$n$ (size of <code>visit_set</code>)</th><th>$t$ (execution time in seconds, 4dp)</th></tr></thead><tbody><tr><td>0</td><td>0.0001</td></tr><tr><td>1</td><td>0.0001</td></tr><tr><td>2</td><td>0.0001</td></tr><tr><td>3</td><td>0.0001</td></tr><tr><td>4</td><td>0.0001</td></tr><tr><td>5</td><td>0.0005</td></tr><tr><td>6</td><td>0.0060</td></tr><tr><td>7</td><td>0.0287</td></tr><tr><td>8</td><td>0.2148</td></tr><tr><td>9</td><td>1.6055</td></tr><tr><td>10</td><td>17.4555</td></tr><tr><td>11</td><td>171.6719</td></tr><tr><td>12</td><td>1750.1065</td></tr></tbody></table><p>Presuming most people&rsquo;s friends live somewhat close to each other, even in the case where we have 10 close friends that we want to hang out with, most of them probably share &ldquo;pickup points&rdquo; which reduces the size of the <code>visit_set</code>. For example, the current input data has 18 friends but a visit set of size 7! This means that in almost every case $n&lt;10$, and if people were using this in a mapping application like Google Maps for example to have certain pickup points along the way, this would most likely be fine, returning a result in a couple seconds at worst.</p><p>The problems start arising when this problem is scaled up more. As the algorithm is in factorial time, it scales rather terribly and has minimal improvements over brute force, if any improvements at all. The algorithm more generally is a solution for TSP with a graph that is not necessarily complete, and this can be applied to a lot more real life applications than just houses of friends. For example, if the person starting the trip was a truck driver for a logistics company rather than me, and the pickup points were necessary delivery points rather than the closest meeting points for friends, we would have a completely different scale in which the algorithm would perform very poorly. Not only would these pickup points be across a <em>much</em> larger distance, meaning the value of $R$ will likely be much higher, but there are potentially many more pickup/dropoff points in a day than the previous scenario, causing both $L$ and $n$ to be greatly larger. Simply put, a factorial time complexity of $O(\lfloor n!(e+1) \rfloor + L(2LR+L^{3}))$ just does not scale very well for many other practical use cases besides the one explored, and even then, if the party of friends was sufficiently large, the algorithm would crawl to a halt. Looking at the example above, with just $12$ pickup points the algorithm ground to a staggering half an hour of required time when tested on my machine.</p><p>Due to the fact that most users are not willing to wait more than a couple seconds for a result, the practical input sizes are $n &lt; 10$, $L \le 15$, $R \le 30$. These values are taken from the input values that produced the table above while considering the time complexity of the algorithm. This is not a very big scope of possible use cases, and therefore optimisations are most definitely needed. Although this algorithm as of now is suitable to the problem&rsquo;s requirements, it very quickly falls apart for a &ldquo;power user&rdquo; or anyone else that has a different use-case in mind. Another possible alternative is using &ldquo;approximate&rdquo; solutions that have a better time complexity which may not provide the <em>most</em> optimal solution, but will most definitely scale better for a variety of use cases.</p><p>To conclude, this algorithm&rsquo;s time complexity directly influences how practically it can be used in the real world to solve the problem it is intended to solve. Users of a program as such would expect a result within seconds at most, and the practical input sizes are therefore restricted to those described above.</p><a href=#appendix><h1 id=appendix><span class=hanchor arialabel=Anchor># </span>Appendix</h1></a><a href=#possible-optimisations><h2 id=possible-optimisations><span class=hanchor arialabel=Anchor># </span>Possible Optimisations</h2></a><p>It is also worth quickly noting the possible optimisations the findings of the report above lead to.</p><ol><li><p>The current implementation of Dijkstra&rsquo;s is far from optimal: the current algorithm has a cubic time complexity but with a a min priority queue this can supposedly be reduced to $O(L+R\log{L})$.</p></li><li><p>The abstraction of <code>soonest_time_at_node</code> can be implemented as a dictionary that is accessed in constant time but is currently implemented as two for loops that makes the
<a rel=noopener class="internal-link broken" data-src=#distance-function><code>dist</code></a> function more complex than necessary.</p></li><li><p>The biggest optimisation needed is the caching of the Held-Karp outputs, meaning that subpaths are calculated once only, and all subsequent subpaths will be read in $O(1)$ time (basically dynamic programming by definition). This should probably help the factorial time complexity, though it might be hindered by the fact that a different starting time means that the whole subpath is different which decreases how effective this optimisation is.</p></li><li><p>Finally, it may be worth considering approximate solutions. This being said, the scope of the problem to solve does <em>just</em> fit into the practical input sizes that the algorithm allows, but definitely limits its usefulness and real world use cases. In many times, the <em>best</em> solution is not needed, just a relatively good one.</p></li></ol><a href=#algorithm-pseudocode><h2 id=algorithm-pseudocode><span class=hanchor arialabel=Anchor># </span>Algorithm Pseudocode</h2></a><p>The following is the final pseudocode reiterated from Part 1, namely for convenience while analysing, since multiple modifications were made to the initial pseudocode.</p><p>Let $A =$ starting vertex
Let $B =$ ending vertex
Let $S = {P, Q, R}$ or any other vertices to be visited along the way.
Let $C \in S$ (random node in $S$)</p><a href=#main-function><h3 id=main-function><span class=hanchor arialabel=Anchor># </span>Main Function</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>function main(
</span></span><span class=line><span class=cl>	friends: dictionary,
</span></span><span class=line><span class=cl>	landmarks: dictionary,
</span></span><span class=line><span class=cl>	routes: dictionary,
</span></span><span class=line><span class=cl>	timetable: dictionary
</span></span><span class=line><span class=cl>):
</span></span><span class=line><span class=cl>	// global variable declarations
</span></span><span class=line><span class=cl>	concession: bool = Ask the user &#34;Do you posses a concession card?&#34;
</span></span><span class=line><span class=cl>	holiday: bool = Ask the user &#34;Is today a weekend or a holiday?&#34;
</span></span><span class=line><span class=cl>	user_name: string = Ask the user to select a friend from friends dictionary
</span></span><span class=line><span class=cl>	selected_time = Ask the user what time they are leaving
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	cached_djk: dictionary = empty dictionary
</span></span><span class=line><span class=cl>	edge_lookup_matrix: matrix = |V| x |V| matrix that stores a list of edges in each entry
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	// get distance of all friends from landmarks
</span></span><span class=line><span class=cl>	friend_distances: dictionary = calculate_nodes(friends, landmarks)
</span></span><span class=line><span class=cl>	visit_set: set = set of all closest nodes from friend_distances
</span></span><span class=line><span class=cl>	people_at_nodes: dictionary = all friends sorted into keys of which nodes they are closest to, from visit_set
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	home: string = closest node of user_name
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	print all friends, where they live closest to and how far away
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	print out friends that would take more than 20 minutes to walk (average human walking speed is 5.1 km/h)
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	hamiltonian_path = held_karp(home, home, visit_set, selected_time)
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	print how much the trip would cost and how long it would take
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	print the path of the hamiltonian_path
</span></span><span class=line><span class=cl>end function
</span></span></code></pre></td></tr></table></div></div><a href=#calculate-nodes><h3 id=calculate-nodes><span class=hanchor arialabel=Anchor># </span>Calculate Nodes</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>function calculate_nodes (
</span></span><span class=line><span class=cl>	friend_data: dictionary,
</span></span><span class=line><span class=cl>	node_data: dictionary
</span></span><span class=line><span class=cl>):
</span></span><span class=line><span class=cl>	for friend in friend_data:
</span></span><span class=line><span class=cl>		home: tuple = friend[&#39;home&#39;]
</span></span><span class=line><span class=cl>		// initial min vals that will be set to smallest iterated distance
</span></span><span class=line><span class=cl>		min: float = infinity
</span></span><span class=line><span class=cl>		min_node: node = null
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		for node in node_data:
</span></span><span class=line><span class=cl>			location: tuple = node[&#39;coordinates&#39;]
</span></span><span class=line><span class=cl>			// find real life distance (functional abstraction)
</span></span><span class=line><span class=cl>			distance: float = latlong_distance(home, location)
</span></span><span class=line><span class=cl>			if distance &lt; min:
</span></span><span class=line><span class=cl>				min = distance
</span></span><span class=line><span class=cl>				min_node = node
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		distance_dict[friend][&#39;min_node&#39;] = min_node
</span></span><span class=line><span class=cl>		distance_dict[friend][&#39;distance&#39;] = min
</span></span><span class=line><span class=cl>end function
</span></span></code></pre></td></tr></table></div></div><a href=#held-karp><h3 id=held-karp><span class=hanchor arialabel=Anchor># </span>Held-Karp</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>function held_karp (
</span></span><span class=line><span class=cl>    start: node,
</span></span><span class=line><span class=cl>    end: node,
</span></span><span class=line><span class=cl>    visit: set&lt;node&gt;,
</span></span><span class=line><span class=cl>    current_time: datetime
</span></span><span class=line><span class=cl>):
</span></span><span class=line><span class=cl>    if visit.size = 0:
</span></span><span class=line><span class=cl>    	djk = dijkstras(start, end, current_time)
</span></span><span class=line><span class=cl>		return djk[&#39;cost&#39;]
</span></span><span class=line><span class=cl>    else:
</span></span><span class=line><span class=cl>        min = infinity
</span></span><span class=line><span class=cl>        For node C in set S:
</span></span><span class=line><span class=cl>	        sub_path = held_karp(start, C, (set \ C), current_time)
</span></span><span class=line><span class=cl>	        djk = dijkstras(C, end, current_time + toMinutes(sub_path[&#39;cost&#39;]))
</span></span><span class=line><span class=cl>	        cost = sub_path[&#39;cost&#39;] + djk[&#39;cost&#39;]
</span></span><span class=line><span class=cl>	        if cost &lt; min:
</span></span><span class=line><span class=cl>	            min = cost
</span></span><span class=line><span class=cl>	    return min
</span></span><span class=line><span class=cl>end function
</span></span></code></pre></td></tr></table></div></div><a href=#dijkstras><h3 id=dijkstras><span class=hanchor arialabel=Anchor># </span>Dijkstra&rsquo;s</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>function dijkstras (
</span></span><span class=line><span class=cl>    start: node,
</span></span><span class=line><span class=cl>    end: node,
</span></span><span class=line><span class=cl>    current_time: datetime
</span></span><span class=line><span class=cl>):
</span></span><span class=line><span class=cl>    // Set all node distance to infinity
</span></span><span class=line><span class=cl>    for node in graph:
</span></span><span class=line><span class=cl>        distance[node] = infinity
</span></span><span class=line><span class=cl>        predecessor[node] = null
</span></span><span class=line><span class=cl>        unexplored_list.add(node)
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    // starting distance has to be 0
</span></span><span class=line><span class=cl>    distance[start] = 0
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    // while more to still explore
</span></span><span class=line><span class=cl>    while unexplored_list is not empty:
</span></span><span class=line><span class=cl>        min_node = unexplored node with min cost
</span></span><span class=line><span class=cl>        unexplored_list.remove(min_node)
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>	    // go through every neighbour and relax
</span></span><span class=line><span class=cl>        for each neighbour of min_node:
</span></span><span class=line><span class=cl>            current_dist = distance[min_node] + dist(min_node, neighbour, current_time + to_minutes(distance[min_node]))
</span></span><span class=line><span class=cl>            // a shorter path has been found to the neighbour -&gt; relax value
</span></span><span class=line><span class=cl>            if current_dist &lt; distance[neighbour]:
</span></span><span class=line><span class=cl>                distance[neighbour] = current_dist
</span></span><span class=line><span class=cl>                predecessor[neighbour] = min_node
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    return distance[end]
</span></span><span class=line><span class=cl>end function
</span></span></code></pre></td></tr></table></div></div><a href=#distance-function><h3 id=distance-function><span class=hanchor arialabel=Anchor># </span>Distance Function</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>function dist (
</span></span><span class=line><span class=cl>	start: node,
</span></span><span class=line><span class=cl>	end: node,
</span></span><span class=line><span class=cl>	current_time: datetime
</span></span><span class=line><span class=cl>):	
</span></span><span class=line><span class=cl>	// if the start and end node are the same, it takes no time to get there
</span></span><span class=line><span class=cl>	if start = end:
</span></span><span class=line><span class=cl>		return 0
</span></span><span class=line><span class=cl>	else if edges = null:
</span></span><span class=line><span class=cl>		// if no edge exists between nodes
</span></span><span class=line><span class=cl>		return infinity
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	edges = edge_lookup_matrix[start][end]
</span></span><span class=line><span class=cl>	distances = []
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	// go over each possible edge between nodes (multiple possible)
</span></span><span class=line><span class=cl>	for edge in edges:
</span></span><span class=line><span class=cl>		line = edge.line
</span></span><span class=line><span class=cl>		// next time bus/train will be at node (functional abstraction)
</span></span><span class=line><span class=cl>		next_time = soonest_time_at_node(timetable, line, start, current_time)
</span></span><span class=line><span class=cl>		wait_time = next_time - current_time
</span></span><span class=line><span class=cl>		distances.add(edge.weight + wait_time)
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	return min(distances)
</span></span><span class=line><span class=cl>end function
</span></span></code></pre></td></tr></table></div></div><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>This analysis is done assuming that the time complexity of accessing a dictionary, list or array element is $O(1)$, as these basic pseudocode elements are generally done in constant time.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Due to the nature of functional abstraction, the implementation of creating the <code>edge_lookup_matrix</code> is not specified in the pseudocode. Although it is referred to as a lookup matrix of size $|V| \times |V|$ which would have a quadratic time complexity, the pseudocode has actually been implemented as a dictionary in $O(2R)$ time, which is a bit more efficient. Nonetheless, even if it was changed to $O(L^{2})$, it would make minimal difference to the final asymptotic time complexity.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>The following variables will be used as shorthand throughout the analysis.</p><p>Let $A =$ starting vertex</p><p>Let $B =$ ending vertex</p><p>Let $S = {P, Q, R}$ or any other vertices to be visited along the way.</p><p>Let $n$ = the length of the visit set $S$.</p><p>Let $C \in S$ (random node in $S$), and to clarify: $C \neq A, B$ as $S$ does not include them&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>Note that $n&lt;5$ would be rather unreliable due to the decimal inaccuracy of my recorded execution times (4dp)&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://quartz.jzhao.xyz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Garv Shah using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://quartz.jzhao.xyz/>Home</a></li><li><a href=https://www.linkedin.com/in/garvshah/>LinkedIn</a></li><li><a href=https://github.com/garv-shah>GitHub</a></li></ul></footer></div></div></body></html>